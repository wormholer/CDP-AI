import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import joblib


class UserBehaviorPredictor:
    """ç”¨æˆ·è¡Œä¸ºé¢„æµ‹AIæ¨¡å‹"""

    def __init__(self):
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.feature_columns = []

    def prepare_features(self, users_df, events_df):
        """å‡†å¤‡æ¨¡å‹ç‰¹å¾"""
        # ç”¨æˆ·åŸºæœ¬ç‰¹å¾
        users_df['is_active'] = (pd.to_datetime(users_df['last_visit']) >
                                 (pd.Timestamp.now() - pd.Timedelta(days=7))).astype(int)

        # è¡Œä¸ºèšåˆç‰¹å¾
        user_behavior = events_df.groupby('user_id').agg({
            'event_type': 'count',
            'value': 'sum',
            'timestamp': lambda x: (pd.Timestamp.now() - pd.to_datetime(x.max())).days
        }).rename(columns={
            'event_type': 'total_events',
            'value': 'total_value',
            'timestamp': 'days_since_last_event'
        })

        # åˆå¹¶ç‰¹å¾
        features_df = users_df.merge(user_behavior, on='user_id', how='left')
        features_df = features_df.fillna(0)

        # ç‰¹å¾å·¥ç¨‹
        features_df['avg_order_value'] = np.where(
            features_df['total_events'] > 0,
            features_df['total_spent'] / features_df['visit_count'],
            0
        )

        # ç¼–ç åˆ†ç±»å˜é‡
        features_df = pd.get_dummies(features_df,
                                     columns=['gender', 'preferred_category', 'region'])

        # é€‰æ‹©ç‰¹å¾åˆ—
        exclude_cols = ['user_id', 'signup_date', 'last_visit']
        self.feature_columns = [col for col in features_df.columns
                                if col not in exclude_cols and not col.startswith('target_')]

        return features_df

    def create_target_variable(self, features_df):
        """åˆ›å»ºé¢„æµ‹ç›®æ ‡ - é«˜ä»·å€¼ç”¨æˆ·"""
        # åŸºäºæ¶ˆè´¹é‡‘é¢å’Œæ´»è·ƒåº¦å®šä¹‰é«˜ä»·å€¼ç”¨æˆ·
        spend_quantile = features_df['total_spent'].quantile(0.7)
        visit_quantile = features_df['visit_count'].quantile(0.7)

        features_df['target_high_value'] = (
                (features_df['total_spent'] > spend_quantile) &
                (features_df['visit_count'] > visit_quantile)
        ).astype(int)

        return features_df

    def train(self, users_df, events_df):
        """è®­ç»ƒé¢„æµ‹æ¨¡å‹"""
        print("ğŸ› ï¸ å‡†å¤‡è®­ç»ƒæ•°æ®...")
        features_df = self.prepare_features(users_df, events_df)
        features_df = self.create_target_variable(features_df)

        X = features_df[self.feature_columns]
        y = features_df['target_high_value']

        # åˆ†å‰²æ•°æ®é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        print("ğŸ¯ è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
        self.model.fit(X_train, y_train)

        # æ¨¡å‹è¯„ä¼°
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

        # ä¿å­˜æ¨¡å‹
        joblib.dump(self.model, 'models/user_value_model.pkl')
        print(f"âœ… æ¨¡å‹ä¿å­˜å®Œæˆ - æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.3f}")

        return {
            "accuracy": accuracy,
            "feature_importance": dict(zip(self.feature_columns,
                                           self.model.feature_importances_))
        }

    def predict(self, user_features):
        """é¢„æµ‹å•ä¸ªç”¨æˆ·ä»·å€¼"""
        prediction = self.model.predict(user_features[self.feature_columns])
        probability = self.model.predict_proba(user_features[self.feature_columns])

        return {
            "is_high_value": prediction[0],
            "probability": probability[0][1],
            "segment": "é«˜ä»·å€¼ç”¨æˆ·" if prediction[0] else "æ™®é€šç”¨æˆ·"
        }